{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c07a6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp==3.9.5 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (3.9.5)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout==4.0.3 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: attrs==23.2.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: certifi==2024.2.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: comet-ml==3.41.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (3.41.0)\n",
      "Requirement already satisfied: comet_llm==2.2.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
      "Requirement already satisfied: configobj==5.0.8 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (5.0.8)\n",
      "Requirement already satisfied: dulwich==0.22.1 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (0.22.1)\n",
      "Requirement already satisfied: everett==3.1.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (3.1.0)\n",
      "Requirement already satisfied: flatten-dict==0.4.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (0.4.2)\n",
      "Requirement already satisfied: frozenlist==1.4.1 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (1.4.1)\n",
      "Requirement already satisfied: idna==3.7 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (3.7)\n",
      "Requirement already satisfied: jsonschema==4.21.1 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-specifications==2023.12.1 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (2023.12.1)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (3.0.0)\n",
      "Requirement already satisfied: mdurl==0.1.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (0.1.2)\n",
      "Requirement already satisfied: multidict==6.0.5 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (6.0.5)\n",
      "Requirement already satisfied: openai==0.28.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (0.28.0)\n",
      "Requirement already satisfied: psutil==5.9.8 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (5.9.8)\n",
      "Requirement already satisfied: Pygments==2.17.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (2.17.2)\n",
      "Requirement already satisfied: python-box==6.1.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (6.1.0)\n",
      "Requirement already satisfied: referencing==0.35.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (0.35.0)\n",
      "Requirement already satisfied: requests==2.31.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt==1.0.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (1.0.0)\n",
      "Requirement already satisfied: rich==13.7.1 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (13.7.1)\n",
      "Requirement already satisfied: rpds-py==0.18.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (0.18.0)\n",
      "Requirement already satisfied: semantic-version==2.10.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk==2.0.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (2.0.0)\n",
      "Requirement already satisfied: simplejson==3.19.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (3.19.2)\n",
      "Requirement already satisfied: six==1.16.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (1.16.0)\n",
      "Requirement already satisfied: tqdm==4.66.2 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (4.66.2)\n",
      "Requirement already satisfied: types-requests==2.31.0.20240406 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (2.31.0.20240406)\n",
      "Requirement already satisfied: urllib3==2.2.1 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (2.2.1)\n",
      "Requirement already satisfied: websocket-client==1.3.3 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (1.3.3)\n",
      "Requirement already satisfied: wrapt==1.16.0 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (1.16.0)\n",
      "Requirement already satisfied: wurlitzer==3.0.3 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (3.0.3)\n",
      "Requirement already satisfied: yarl==1.9.4 in ./.env/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (1.9.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/hsin-wenchang/Documents/GitHub/Project-Build-Custom-Chatbot/.env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3044467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : main_fowl_2314\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/polarbeargo/build-custom-chatbot/ff5d2ea81e5842ce86bfd685b33915fd\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (669.23 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/polarbeargo/build-custom-chatbot/0ebd1eb1a31a4f8d8fa69d15b16e21fe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset\n",
    "import comet_llm\n",
    "import pandas as pd\n",
    "from comet_ml import Experiment\n",
    "from comet_llm import Span, end_chain, start_chain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Union\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = \"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\"\n",
    "COMET_API_KEY = \"9ML6W1iTXa1yVtacY9MtSVQAk\"\n",
    "\n",
    "# Initialize the experiment\n",
    "experiment = Experiment(\n",
    "    api_key=\"9ML6W1iTXa1yVtacY9MtSVQAk\",\n",
    "    project_name=\"build-custom-chatbot\",\n",
    "    workspace=\"polarbeargo\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "In this cell, write an explanation of which dataset you have chosen and why it is appropriate for this task\n",
    "`nyc_food_scrap_drop_off_sites.csv` - This file contains information on food scrap drop-off stations in New York City, such as their locations and hours of operation. The data was obtained in the beginning of 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "In the cells below, load your chosen dataset into a `pandas` dataframe with a column named `\"text\"`. This column should contain all of your text data, separated into at least 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51ffb997",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data\")\n",
    "file_paths = [filename for filename in data_path.glob(\"*.csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c1094e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'data/nyc_food_scrap_drop_off_sites.csv'\n",
    "with open(file_path, 'r') as file:\n",
    "    next(file)\n",
    "    lines = [line.strip() for line in file]\n",
    "\n",
    "df = pd.DataFrame(lines, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "85b1013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0,Staten Island,Grasmere-Arrochar-South Beach-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1,Manhattan,Inwood,SE Corner of Broadway &amp; Aca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2,Brooklyn,Park Slope,Old Stone House Brooklyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3,Manhattan,East Harlem (North),SE Corner of P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4,Queens,Corona,Malcolm X FSDO,\"111-26 Norther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  0,Staten Island,Grasmere-Arrochar-South Beach-...\n",
       "1  1,Manhattan,Inwood,SE Corner of Broadway & Aca...\n",
       "2  2,Brooklyn,Park Slope,Old Stone House Brooklyn...\n",
       "3  3,Manhattan,East Harlem (North),SE Corner of P...\n",
       "4  4,Queens,Corona,Malcolm X FSDO,\"111-26 Norther..."
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d1f17c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/data_wrangling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "df317701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Staten Island,Grasmere-Arrochar-South Beach-Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manhattan,Inwood,SE Corner of Broadway &amp; Acade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brooklyn,Park Slope,Old Stone House Brooklyn,\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manhattan,East Harlem (North),SE Corner of Ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queens,Corona,Malcolm X FSDO,\"111-26 Northern ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Queens,Astoria (North)-Ditmars-Steinway,Astori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bronx,Norwood,SE Corner of Kings College Place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brooklyn,Bedford-Stuyvesant (East),NW Corner o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Queens,Astoria (Central),Astoria Pug: Broadway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bronx,Mount Eden-Claremont (West),SE Corner of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bronx,Fordham Heights,SE Corner of Field Place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Queens,Queensbridge-Ravenswood-Dutch Kills,Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Brooklyn,Crown Heights (North),Walt L Shamel C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Brooklyn,Prospect Heights,Underhill Avenue &amp; P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Manhattan,Morningside Heights,SW Corner of Wes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Queens,Astoria (Central),SW Corner of 33rd Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Queens,Long Island City-Hunters Point,The Conn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brooklyn,Sunset Park (West),Los Colibríes Comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Manhattan,Lower East Side,Grand Street &amp; Clint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bronx,Tremont,La Familia Verde Farmer's Market...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   Staten Island,Grasmere-Arrochar-South Beach-Do...\n",
       "1   Manhattan,Inwood,SE Corner of Broadway & Acade...\n",
       "2   Brooklyn,Park Slope,Old Stone House Brooklyn,\"...\n",
       "3   Manhattan,East Harlem (North),SE Corner of Ple...\n",
       "4   Queens,Corona,Malcolm X FSDO,\"111-26 Northern ...\n",
       "5   Queens,Astoria (North)-Ditmars-Steinway,Astori...\n",
       "6   Bronx,Norwood,SE Corner of Kings College Place...\n",
       "7   Brooklyn,Bedford-Stuyvesant (East),NW Corner o...\n",
       "8   Queens,Astoria (Central),Astoria Pug: Broadway...\n",
       "9   Bronx,Mount Eden-Claremont (West),SE Corner of...\n",
       "10  Bronx,Fordham Heights,SE Corner of Field Place...\n",
       "11  Queens,Queensbridge-Ravenswood-Dutch Kills,Com...\n",
       "12  Brooklyn,Crown Heights (North),Walt L Shamel C...\n",
       "13  Brooklyn,Prospect Heights,Underhill Avenue & P...\n",
       "14  Manhattan,Morningside Heights,SW Corner of Wes...\n",
       "15  Queens,Astoria (Central),SW Corner of 33rd Str...\n",
       "16  Queens,Long Island City-Hunters Point,The Conn...\n",
       "17  Brooklyn,Sunset Park (West),Los Colibríes Comm...\n",
       "18  Manhattan,Lower East Side,Grand Street & Clint...\n",
       "19  Bronx,Tremont,La Familia Verde Farmer's Market..."
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].str.replace(r'^\\d+,', '', regex=True)\n",
    "df.head(20).to_csv(\"data/data_wrangling_sample.csv\")\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6612a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'paraphrase-MiniLM-L6-v2'\n",
    "df = pd.read_csv('data/data_wrangling_sample.csv')\n",
    "\n",
    "def generate_embeddings(input_data: Union[str, list[str]]) -> np.ndarray:    \n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    embeddings = model.encode(input_data)\n",
    "    return embeddings\n",
    "\n",
    "df['embedding'] = df.text.apply(lambda x: generate_embeddings(x))\n",
    "df.to_csv('embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "97acd282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Staten Island,Grasmere-Arrochar-South Beach-Do...</td>\n",
       "      <td>[-3.98153216e-02 -4.14079800e-02  5.66049144e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manhattan,Inwood,SE Corner of Broadway &amp; Acade...</td>\n",
       "      <td>[ 0.13020103 -0.13104136 -0.2690476  -0.235636...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brooklyn,Park Slope,Old Stone House Brooklyn,\"...</td>\n",
       "      <td>[-0.00706969  0.02325399 -0.0220314   0.036282...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manhattan,East Harlem (North),SE Corner of Ple...</td>\n",
       "      <td>[ 1.58163443e-01 -9.84401628e-02 -2.88609684e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Queens,Corona,Malcolm X FSDO,\"111-26 Northern ...</td>\n",
       "      <td>[-0.22042143 -0.17215516 -0.04905648 -0.030325...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Staten Island,Grasmere-Arrochar-South Beach-Do...   \n",
       "1  Manhattan,Inwood,SE Corner of Broadway & Acade...   \n",
       "2  Brooklyn,Park Slope,Old Stone House Brooklyn,\"...   \n",
       "3  Manhattan,East Harlem (North),SE Corner of Ple...   \n",
       "4  Queens,Corona,Malcolm X FSDO,\"111-26 Northern ...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-3.98153216e-02 -4.14079800e-02  5.66049144e-...  \n",
       "1  [ 0.13020103 -0.13104136 -0.2690476  -0.235636...  \n",
       "2  [-0.00706969  0.02325399 -0.0220314   0.036282...  \n",
       "3  [ 1.58163443e-01 -9.84401628e-02 -2.88609684e-...  \n",
       "4  [-0.22042143 -0.17215516 -0.04905648 -0.030325...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"embeddings.csv\")\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion\n",
    "\n",
    "In the cells below, compose a custom query using your chosen dataset and retrieve results from an OpenAI `Completion` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a30fa0e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (384,) and (0,) not aligned: 384 (dim 0) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mfromstring(x[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     22\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the food scrap drop-off site in Brooklyn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m relevant_quote, similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relevant_quote \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     relevant_quote \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(relevant_quote)\n",
      "Cell \u001b[0;32mIn[152], line 30\u001b[0m, in \u001b[0;36mcustom_query\u001b[0;34m(query, dataframe)\u001b[0m\n\u001b[1;32m     28\u001b[0m quotes \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     29\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m generate_embeddings([query])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [cosine_similarity(query_embedding, quote_embedding) \u001b[38;5;28;01mfor\u001b[39;00m quote_embedding \u001b[38;5;129;01min\u001b[39;00m quote_embeddings]\n\u001b[1;32m     31\u001b[0m most_similar_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(similarities)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quotes[most_similar_index], similarities[most_similar_index]\n",
      "Cell \u001b[0;32mIn[152], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m quotes \u001b[38;5;241m=\u001b[39m dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     29\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m generate_embeddings([query])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote_embedding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m quote_embedding \u001b[38;5;129;01min\u001b[39;00m quote_embeddings]\n\u001b[1;32m     31\u001b[0m most_similar_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(similarities)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quotes[most_similar_index], similarities[most_similar_index]\n",
      "Cell \u001b[0;32mIn[152], line 20\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine_similarity\u001b[39m(vec1, vec2):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(vec1) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(vec2))\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (384,) and (0,) not aligned: 384 (dim 0) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "def get_basic_completion(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt[:16300]}],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=300):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def custom_query(query, dataframe):\n",
    "   \n",
    "    if dataframe['embedding'].empty:\n",
    "        return None, None\n",
    "    \n",
    "    quote_embeddings = dataframe['embedding'].tolist()\n",
    "    quotes = dataframe['text'].tolist()\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    similarities = [cosine_similarity(query_embedding, quote_embedding) for quote_embedding in quote_embeddings]\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "    \n",
    "    return quotes[most_similar_index], similarities[most_similar_index]\n",
    "\n",
    "query = \"What is the food scrap drop-off site in Brooklyn\"\n",
    "\n",
    "# Convert df['embedding'] from string to list of floats\n",
    "df['embedding'] = df['embedding'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
    "relevant_quote, similarity_score = custom_query(query, df)\n",
    "\n",
    "if relevant_quote is not None:\n",
    "    relevant_quote = str(relevant_quote)\n",
    "    answer = get_completion(relevant_quote, query)\n",
    "\n",
    "    print(f\"Relevant Quote: {relevant_quote}\")\n",
    "    print(f\"Similarity Score: {similarity_score}\")\n",
    "    print(f\"OpenAI Answer: {answer}\")\n",
    "else:\n",
    "    print(\"No relevant quote found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cb7a9",
   "metadata": {},
   "source": [
    "The method below retrieves the full response (by chain-of-thought) before extracting the final response based on the user's question. Using Comet's prompt chains logging features, it logs the final response and the CoT findings for each question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration\n",
    "\n",
    "TODO: In the cells below, demonstrate the performance of your custom query using at least 2 questions. For each question, show the answer from a basic `Completion` model query as well as the answer from your custom query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()\n",
    "random_row = df.sample(n=1)\n",
    "text = random_row['text'].values[0][:16300]\n",
    "\n",
    "basic_prompt = \"What is the food scrap drop-off site in Brooklyn?\"\n",
    "basic_response = get_basic_completion(basic_prompt[:16300])  \n",
    "\n",
    "experiment.log_text(\"Custom Prompt\", text)\n",
    "experiment.log_text(\"Custom Response\", answer)\n",
    "experiment.log_text(\"Basic Prompt\", basic_prompt)\n",
    "experiment.log_text(\"Basic Response\", basic_response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Sentence: ,borough,ntaname,food_scrap_drop_off_site,location,hosted_by,open_months,operation_day_hours,website,borocd,councildist,latitude,longitude,precinct,object_id,location_point,:@computed_region_yeji_bk3q,:@computed_region_92fq_4b7q,:@computed_region_sbqj_enih,:@computed_region_efsh_h5xi,:@computed_region_f5dn_yrer,notes,ct2010,bbl,bin\n",
    "0,Staten Island,Grasmere-Arrochar-South Beach-Dongan Hills,South Beach,\"21 Robin Road, Staten Island NY\",Snug Harbor Youth,Year Round,Friday (Start Time: 1:30 PM - End Time:  4:30 PM),snug-harbor.org,502,50,40.595579,-74.062991,122,3842,\"{'type': 'Point', 'coordinates': [-74.062991, 40.595579]}\",1.0,14.0,76.0,10692.0,30.0,,,,\n",
    "1,Manhattan,Inwood,SE Corner of Broadway & Academy Street,,Department of Sanitation,Year Round,24/7,www.nyc.gov/smartcomposting,112,10,,,34,3573,,,,,,,\"Download the app to access bins. Accepts all food scraps, including meat and dairy. Do not leave food scraps outside of bin!\",,,\n",
    "2,Brooklyn,Park Slope,Old Stone House Brooklyn,\"336 3rd St, Brooklyn, NY 11215\",Old Stone House Brooklyn,Year Round,24/7 (Start Time: 24/7 - End Time:  24/7),,306,39,40.6727118,-73.984731,78,3555,\"{'type': 'Point', 'coordinates': [-73.984731, 40.6727118]}\",2.0,27.0,50.0,17617.0,14.0,,,,\n",
    "3,Manhattan,East Harlem (North),SE Corner of Pleasant Avenue & E 116 Street,,Department of Sanitation,Year Round,24/7,www.nyc.gov/smartcomposting,111,8,,,25,3438,,,,,,,\"Download the app to access bins. Accepts all food scraps, including meat and dairy. Do not leave food scraps outside of bin!\",,,\n",
    "4,Queens,Corona,Malcolm X FSDO,\"111-26 Northern Blvd, Flushing, NY 11368\",NYC Compost Project Hosted by Big Reuse,Year Round,Tuesdays (Start Time: 12:00 PM - End Time:  2:00 PM),,404,21,40.7496855,-73.8630721,110,3388,\"{'type': 'Point', 'coordinates': [-73.8630721, 40.7496855]}\",3.0,21.0,68.0,14510.0,66.0,,,,\n",
    "5,Queens,Astoria (North)-Ditmars-Steinway,Astoria Pug: 41st Street,Ditmars Boulevard and 41st Street,Astoria Pug,Year Round,Mondays (Start Time: 8:00 AM - End Time:  2:00 PM),https://www.instagram.com/astoriapug/?hl=en,401,22,40.7724122,-73.9053388,114,3350,\"{'type': 'Point', 'coordinates': [-73.9053388, 40.7724122]}\",3.0,4.0,72.0,16862.0,39.0,\"Not accepted: meat, bones, or dairy\",119.0,,\n",
    "6,Bronx,Norwood,SE Corner of Kings College Place & Gun Hill Rd.,,Department of Sanitation,Year Round,24/7,www.nyc.gov/smartcomposting,207,11,,,52,3773,,,,,,,\"Download the app to access bins. Accepts all food scraps, including meat and dairy. Do not leave food scraps outside of bin!\",,,\n",
    "7,Brooklyn,Bedford-Stuyvesant (East),NW Corner of Malcolm X Boulevard & Bainbridge Street,,Department of Sanitation,Year Round,24/7,www.nyc.gov/smartcomposting,303,36,,,81,3477,,,,,,,\"Download the app to access bins. Accepts all food scraps, including meat and dairy. Do not leave food scraps outside of bin!\",,,\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of questions\n",
    "questions = [\n",
    "    \"What is the food scrap drop-off site in Brooklyn?\",\n",
    "    \"Where can I drop off food scraps in Manhattan?\",\n",
    "    \"Are there any food scrap drop-off sites in Queens?\",\n",
    "    \"What are the hours of operation for food scrap drop-off sites in the Bronx?\",\n",
    "    \"Can I drop off food scraps in Staten Island?\",\n",
    "    \"Is there a food scrap drop-off site near me?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chatbot_responses = []\n",
    "\n",
    "def get_only_response(response):\n",
    "  messages = [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"Your task is to extract only the response to the user in the following full chatbot response: {response}\".format(response=response)\n",
    "    }\n",
    "  ]\n",
    "\n",
    "  return get_completion(messages)\n",
    "\n",
    "for question in questions:\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"Your task is to answer questions factually about a nyc food scrap drop off sites, provided below and delimited by +++++. The user request is provided here: {request}\\n\\nStep 1: The first step is to check if the user is asking a question related to any type of food scrap drop off sites (even if that food scrap drop off sites is not on the list). If the question is about any type of food scrap drop off sites, we move on to Step 2 and ignore the rest of Step 1. If the question is not about food scrap drop off sites, then you send a response: \\\"Sorry! I cannot help with that. Please let me know if you have a question about our food scrap drop off sites.\\\"\\n\\nStep 2: In this step, you check that the user question is relevant to any of the items on the food scrap drop off sites. You should check that the food scrap drop off site exists in the food scrap drop off sites. If it doesn't exist then send a kind response to the user that the item doesn't exist in the exsisting food scrap drop off sites and then include a list of available but similar food scrap drop off sites without any other details (e.g., location). The food scrap drop off sites available are provided below and delimited by +++++: {location}+++++\\n\\nStep 3: If the item exists in the food scrap drop off sites and the user is requesting specific information, provide that relevant information to the user using the food scrap drop off sites. Make sure to use a friendly tone and keep the response concise.\\n\\nPerform the following reasoning steps to send a response to the user:\\nStep 1: <Step 1 reasoning>\\nStep 2: <Step 2 reasoning>\\nResponse to the user (only output the final response): <response to user>\".format(request=question, food_scrap_drop_off_sites=prompt)\n",
    "    }\n",
    "  ]\n",
    "\n",
    "  response = get_completion(messages)\n",
    "  chatbot_responses.append(response)\n",
    "\n",
    "  start_chain(\n",
    "    inputs={\"question\": question},\n",
    "    api_key=COMET_API_KEY,\n",
    "  )\n",
    "\n",
    "  with Span(\n",
    "    category=\"reasoning\",\n",
    "    name=\"chain-of-thought\",\n",
    "    inputs={\"user_question\": question},\n",
    "    ) as span:\n",
    "      span.set_outputs(outputs={\"full_response\": response})\n",
    "\n",
    "  with Span(\n",
    "    category=\"response-extraction\",\n",
    "    inputs={\n",
    "        \"user_question\": question,\n",
    "        \"full_response\": response,\n",
    "    },\n",
    "  ) as span:\n",
    "    final_response = get_only_response(response)\n",
    "    span.set_outputs(outputs={\"final_response\": final_response})\n",
    "\n",
    "  end_chain(outputs={\"final_response\": final_response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def evaluate_response(response):\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    response_embedding = generate_embeddings([response])[0]\n",
    "    similarity_score = cosine_similarity(query_embedding.reshape(1, -1), response_embedding.reshape(1, -1))\n",
    "    return similarity_score[0][0]\n",
    "\n",
    "for question in questions:\n",
    "   \n",
    "    custom_prompt = question\n",
    "    custom_response = get_basic_completion(custom_prompt[:16300])\n",
    "    custom_relevance = evaluate_response(custom_response.choices[0].message['content'])\n",
    "    \n",
    "    basic_response = get_basic_completion(question[:16300])\n",
    "    basic_relevance = evaluate_response(basic_response.choices[0].message['content'])\n",
    "    \n",
    "    experiment.log_metric(\"Custom Relevance Score\", custom_relevance)\n",
    "    experiment.log_metric(\"Basic Relevance Score\", basic_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a960b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : angry_drywall_8219\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/polarbeargo/build-custom-chatbot/29a209d1be5746dfb71fdc9acd24c133\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (13.09 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
